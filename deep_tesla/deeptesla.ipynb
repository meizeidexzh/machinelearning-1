{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: C:\\machinelearning\\epochs\\epoch01_front.mkv\n",
      "Shape:x = (10, 64, 64, 3) y = (10, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1cb7fa0b7f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nO3AMCuIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb7e1c4da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import params,utils\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#按批次读取数据\n",
    "class VidDataGenerator():\n",
    "    def __init__(self,img_path,batch_id,batch_size=None,input_shape=(224,224,3),train_ratio=1):\n",
    "        self.img_path = img_path\n",
    "        self.batch_id = batch_id\n",
    "        self.batch_size = batch_size\n",
    "        self.current_frame = 0\n",
    "        self.input_shape = input_shape\n",
    "        self.train_ratio = train_ratio\n",
    "        self.vid_path = utils.join_dir(params.data_dir, 'epoch{:0>2}_front.mkv'.format(self.batch_id))\n",
    "        print('Training:',self.vid_path)\n",
    "        \n",
    "    def next_batch(self):\n",
    "        #标注处理\n",
    "        label_all = utils.get_human_steering(self.batch_id)\n",
    "        if(self.batch_size == None):\n",
    "            self.batch_size = len(label_all)\n",
    "            \n",
    "        label = utils.get_human_steering(self.batch_id)[self.current_frame:self.current_frame+self.batch_size]\n",
    "        labels = [[label[i]] for i in range(len(label))]\n",
    "        labels = np.array(labels)\n",
    "        #图像处理        \n",
    "        \n",
    "        cap = cv2.VideoCapture(self.vid_path)\n",
    "        nframe = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) - 2)\n",
    "        if( (self.current_frame+self.batch_size) > nframe):\n",
    "            self.batch_size = nframe - self.current_frame\n",
    "        images = []\n",
    "        for i in range(self.current_frame,self.current_frame+self.batch_size):\n",
    "            utils.cv2_goto_frame(cap,i) \n",
    "            ret, frame = cap.read()\n",
    "            if (ret == True):\n",
    "                shape = frame.shape\n",
    "                frame = frame[int(shape[0]/3):shape[0]-150, 0:shape[1]]\n",
    "                frame = cv2.resize(frame, (self.input_shape[0], self.input_shape[1]), interpolation=cv2.INTER_AREA)\n",
    "                frame = np.resize(frame, (self.input_shape[0], self.input_shape[1], self.input_shape[2]))\n",
    "                images.append(frame)\n",
    "                del frame\n",
    "        images = np.array(images)    \n",
    "        self.current_frame = utils.cv2_current_frame(cap)\n",
    "        cap.release()\n",
    "\n",
    "        return (images[:int(self.batch_size*self.train_ratio)],labels[:int(self.batch_size*self.train_ratio)],\n",
    "                images[int(self.batch_size*self.train_ratio):],labels[int(self.batch_size*self.train_ratio):])\n",
    "    \n",
    "#Test\n",
    "from matplotlib import pyplot \n",
    "%matplotlib inline\n",
    "img_path = params.data_dir\n",
    "input_shape=(64, 64, params.FLAGS.img_c)\n",
    "x_test,y_test,_,_ = VidDataGenerator(img_path,'01',batch_size=10,input_shape=input_shape).next_batch()\n",
    "assert len(x_test)==len(y_test)\n",
    "index = 9\n",
    "print('Shape:x =',x_test.shape,'y =',y_test.shape)\n",
    "pyplot.imshow(x_test[index])\n",
    "pyplot.title('Angle:'+str(y_test[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#生成器\n",
    "def train_generator(img_path,epochs):\n",
    "    for i in epochs:\n",
    "        try:\n",
    "            file_name = utils.join_dir(img_path, 'epoch{:0>2}.pkl'.format(i))\n",
    "            with open(file_name,'rb') as fs:\n",
    "                x_train,y_train = pickle.load(fs)\n",
    "                yield x_train,y_train\n",
    "        except StopIteration as e:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#生成测试文件\n",
    "def save_train_file():\n",
    "    img_path = params.data_dir\n",
    "    batch_size = None\n",
    "    input_shape=(params.FLAGS.img_w, params.FLAGS.img_h, params.FLAGS.img_c)\n",
    "    #for i in ['01','02','03','04','05','06','07','08','09','10']:\n",
    "    for i in ['02']:\n",
    "        file_name = utils.join_dir(img_path, 'epoch{:0>2}.pkl'.format(i))\n",
    "        if(os.path.exists(file_name)):\n",
    "            print(file_name,'Already exist')\n",
    "            break\n",
    "        train_data= VidDataGenerator(img_path=img_path,batch_id=i,batch_size=batch_size,input_shape=input_shape)\n",
    "        x_train,y_train,_,_ = train_data.next_batch()\n",
    "        \n",
    "        with open(file_name,'wb') as fs:\n",
    "            pickle.dump((x_train,y_train),fs)\n",
    "        print('Save '+ file_name)\n",
    "        \n",
    "save_train_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vgg16模型改造\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.layers import Lambda\n",
    "\n",
    "def vgg16_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    #lambda_input = Lambda(lambda x:x/255,name='Lambda')(inputs)\n",
    "    base_model = VGG16(include_top=False, weights='imagenet',input_tensor=inputs)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(1164, activation='relu', name='fc1')(x)\n",
    "    x = Dropout(0.5,name='fc1_dropout')(x)\n",
    "    x = Dense(100, activation='relu', name='fc2')(x)\n",
    "    x = Dropout(0.5,name='fc2_dropout')(x)\n",
    "    x = Dense(50, activation='relu', name='fc3')(x)\n",
    "    x = Dropout(0.5,name='fc3_dropout')(x)\n",
    "    x = Dense(10, activation='relu', name='fc4')(x)\n",
    "    x = Dropout(0.5,name='fc4_dropout')(x)\n",
    "    x = Dense(1,name='output')(x)\n",
    "    \n",
    "    model = Model(base_model.input,x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input,Conv2D,Lambda,MaxPooling2D,Flatten,Dense\n",
    "def nvidia_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = Lambda(lambda x:x/255,name='Lambda')(inputs)\n",
    "    \n",
    "    x = Conv2D(24, (5, 5), activation='relu', padding='same', name='block1_conv1')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    x = Conv2D(36, (5, 5), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "    \n",
    "    x = Conv2D(48, (5, 5), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "    \n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(1164, activation='relu', name='fc1')(x)\n",
    "    x = Dropout(0.5,name='fc1_dropout')(x)\n",
    "    x = Dense(100, activation='relu', name='fc2')(x)\n",
    "    x = Dropout(0.5,name='fc2_dropout')(x)\n",
    "    x = Dense(50, activation='relu', name='fc3')(x)\n",
    "    x = Dropout(0.5,name='fc3_dropout')(x)\n",
    "    x = Dense(10, activation='relu', name='fc4')(x)\n",
    "    x = Dropout(0.5,name='fc4_dropout')(x)\n",
    "    x = Dense(1,name='output')(x)\n",
    "    \n",
    "    model = Model(inputs,x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Convolution2D\n",
    "from keras.models import Sequential\n",
    "def nvidia_model2(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x:x/255, input_shape=input_shape))\n",
    "    model.add(Convolution2D(24,5,5,border_mode='valid', activation='relu', subsample=(2,2)))\n",
    "    model.add(Convolution2D(36,5,5,border_mode='valid', activation='relu', subsample=(2,2)))\n",
    "    model.add(Convolution2D(48,5,5,border_mode='valid', activation='relu', subsample=(2,2)))\n",
    "    model.add(Convolution2D(64,3,3,border_mode='valid', activation='relu', subsample=(1,1)))\n",
    "    model.add(Convolution2D(64,3,3,border_mode='valid', activation='relu', subsample=(1,1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1164, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练模型\n",
    "from keras import optimizers\n",
    "import h5py\n",
    "\n",
    "img_path = params.data_dir\n",
    "batch_size = None\n",
    "input_shape=(params.FLAGS.img_w, params.FLAGS.img_h, params.FLAGS.img_c)\n",
    "model = nvidia_model(input_shape=input_shape)\n",
    "sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "               optimizer=adam,\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(train_generator(img_path,['01']),\n",
    "                    steps_per_epoch=10,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "Lambda (Lambda)              (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 24)        1824      \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 24)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 36)        21636     \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 36)        0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 48)        43248     \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 64)          27712     \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 1164)              299148    \n",
      "_________________________________________________________________\n",
      "fc1_dropout (Dropout)        (None, 1164)              0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 100)               116500    \n",
      "_________________________________________________________________\n",
      "fc2_dropout (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "fc3_dropout (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "fc4 (Dense)                  (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "fc4_dropout (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 552,567\n",
      "Trainable params: 552,567\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1500 samples, validate on 2700 samples\n",
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 19s 12ms/step - loss: 11.1470 - acc: 0.0627 - val_loss: 3.0348 - val_acc: 0.0885\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 10.9631 - acc: 0.0607 - val_loss: 3.5062 - val_acc: 0.0885\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 10.7237 - acc: 0.0673 - val_loss: 4.3616 - val_acc: 0.0348\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 10.6927 - acc: 0.0647 - val_loss: 5.0768 - val_acc: 0.0348\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 10.6840 - acc: 0.0760 - val_loss: 5.1303 - val_acc: 0.0348\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 10.7103 - acc: 0.0693 - val_loss: 4.7404 - val_acc: 0.0348\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 17s 12ms/step - loss: 10.5707 - acc: 0.0760 - val_loss: 4.7637 - val_acc: 0.0348\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 17s 11ms/step - loss: 10.6447 - acc: 0.0780 - val_loss: 4.6024 - val_acc: 0.0348\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 17s 11ms/step - loss: 10.5269 - acc: 0.0720 - val_loss: 4.2861 - val_acc: 0.0348\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 17s 11ms/step - loss: 10.4487 - acc: 0.0727 - val_loss: 3.7485 - val_acc: 0.0922\n",
      "Train on 3900 samples, validate on 2700 samples\n",
      "Epoch 1/10\n",
      "3900/3900 [==============================] - 34s 9ms/step - loss: 3.8725 - acc: 0.1062 - val_loss: 3.0730 - val_acc: 0.0885\n",
      "Epoch 2/10\n",
      "3900/3900 [==============================] - 34s 9ms/step - loss: 3.6773 - acc: 0.1103 - val_loss: 2.7852 - val_acc: 0.0885\n",
      "Epoch 3/10\n",
      "3900/3900 [==============================] - 34s 9ms/step - loss: 3.3046 - acc: 0.1162 - val_loss: 2.4939 - val_acc: 0.1004\n",
      "Epoch 4/10\n",
      "3900/3900 [==============================] - 34s 9ms/step - loss: 3.0455 - acc: 0.1159 - val_loss: 2.5255 - val_acc: 0.0741\n",
      "Epoch 5/10\n",
      "3900/3900 [==============================] - 36s 9ms/step - loss: 2.8269 - acc: 0.1231 - val_loss: 2.7943 - val_acc: 0.0667\n",
      "Epoch 6/10\n",
      "3900/3900 [==============================] - 36s 9ms/step - loss: 2.7151 - acc: 0.1190 - val_loss: 2.0098 - val_acc: 0.0963\n",
      "Epoch 7/10\n",
      "3900/3900 [==============================] - 35s 9ms/step - loss: 2.6306 - acc: 0.1228 - val_loss: 2.9138 - val_acc: 0.0681\n",
      "Epoch 8/10\n",
      "3900/3900 [==============================] - 35s 9ms/step - loss: 2.5688 - acc: 0.1267 - val_loss: 2.0472 - val_acc: 0.0952\n",
      "Epoch 9/10\n",
      "3900/3900 [==============================] - 34s 9ms/step - loss: 2.5332 - acc: 0.1274 - val_loss: 2.1945 - val_acc: 0.0878\n",
      "Epoch 10/10\n",
      "3900/3900 [==============================] - 34s 9ms/step - loss: 2.5274 - acc: 0.1336 - val_loss: 2.4310 - val_acc: 0.0859\n",
      "Train on 2700 samples, validate on 2700 samples\n",
      "Epoch 1/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 6.6202 - acc: 0.1293 - val_loss: 1.4959 - val_acc: 0.1026\n",
      "Epoch 2/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 6.6133 - acc: 0.1315 - val_loss: 1.4331 - val_acc: 0.1007\n",
      "Epoch 3/10\n",
      "2700/2700 [==============================] - 25s 9ms/step - loss: 6.4719 - acc: 0.1322 - val_loss: 1.8794 - val_acc: 0.1130\n",
      "Epoch 4/10\n",
      "2700/2700 [==============================] - 25s 9ms/step - loss: 6.4153 - acc: 0.1370 - val_loss: 1.7952 - val_acc: 0.1111\n",
      "Epoch 5/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 6.3230 - acc: 0.1385 - val_loss: 1.5363 - val_acc: 0.1085\n",
      "Epoch 6/10\n",
      "2700/2700 [==============================] - 25s 9ms/step - loss: 6.4085 - acc: 0.1378 - val_loss: 1.8003 - val_acc: 0.1104\n",
      "Epoch 7/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 6.2702 - acc: 0.1463 - val_loss: 1.6278 - val_acc: 0.1111\n",
      "Epoch 8/10\n",
      "2700/2700 [==============================] - 25s 9ms/step - loss: 6.1938 - acc: 0.1404 - val_loss: 1.8881 - val_acc: 0.1115\n",
      "Epoch 9/10\n",
      "2700/2700 [==============================] - 25s 9ms/step - loss: 6.1000 - acc: 0.1530 - val_loss: 1.5615 - val_acc: 0.1133\n",
      "Epoch 10/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 6.1786 - acc: 0.1530 - val_loss: 1.4584 - val_acc: 0.1052\n",
      "Train on 2700 samples, validate on 2700 samples\n",
      "Epoch 1/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 67.7218 - acc: 0.0193 - val_loss: 1.7403 - val_acc: 0.1019\n",
      "Epoch 2/10\n",
      "2700/2700 [==============================] - 27s 10ms/step - loss: 63.1451 - acc: 0.0219 - val_loss: 3.0213 - val_acc: 0.0752\n",
      "Epoch 3/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 61.2191 - acc: 0.0211 - val_loss: 3.0314 - val_acc: 0.0752\n",
      "Epoch 4/10\n",
      "2700/2700 [==============================] - 26s 9ms/step - loss: 61.0687 - acc: 0.0211 - val_loss: 3.0153 - val_acc: 0.0748\n",
      "Epoch 5/10\n",
      "2700/2700 [==============================] - 27s 10ms/step - loss: 60.7229 - acc: 0.0226 - val_loss: 2.8124 - val_acc: 0.0756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "2700/2700 [==============================] - 27s 10ms/step - loss: 60.5107 - acc: 0.0241 - val_loss: 2.9895 - val_acc: 0.0744\n",
      "Epoch 7/10\n",
      "2700/2700 [==============================] - 26s 9ms/step - loss: 60.7468 - acc: 0.0241 - val_loss: 2.7528 - val_acc: 0.0767\n",
      "Epoch 8/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 60.3982 - acc: 0.0263 - val_loss: 2.7494 - val_acc: 0.0752\n",
      "Epoch 9/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 60.5960 - acc: 0.0296 - val_loss: 2.8383 - val_acc: 0.0748\n",
      "Epoch 10/10\n",
      "2700/2700 [==============================] - 26s 9ms/step - loss: 60.5850 - acc: 0.0293 - val_loss: 2.9699 - val_acc: 0.0759\n",
      "Train on 2700 samples, validate on 2700 samples\n",
      "Epoch 1/10\n",
      "2700/2700 [==============================] - 26s 9ms/step - loss: 21.8170 - acc: 0.0337 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 2/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 19.6142 - acc: 0.0315 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 3/10\n",
      "2700/2700 [==============================] - 25s 9ms/step - loss: 19.6404 - acc: 0.0322 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 4/10\n",
      "2700/2700 [==============================] - 26s 9ms/step - loss: 19.7819 - acc: 0.0319 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 5/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 19.5180 - acc: 0.0281 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 6/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 19.5906 - acc: 0.0356 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 7/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 19.5114 - acc: 0.0311 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 8/10\n",
      "2700/2700 [==============================] - 25s 9ms/step - loss: 19.5091 - acc: 0.0281 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 9/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 19.5564 - acc: 0.0293 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 10/10\n",
      "2700/2700 [==============================] - 27s 10ms/step - loss: 19.5612 - acc: 0.0285 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Train on 2700 samples, validate on 2700 samples\n",
      "Epoch 1/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 43.7627 - acc: 0.0444 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 2/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 43.7381 - acc: 0.0459 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 3/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 43.6959 - acc: 0.0470 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 4/10\n",
      "2700/2700 [==============================] - 27s 10ms/step - loss: 43.7070 - acc: 0.0444 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 5/10\n",
      "2700/2700 [==============================] - 27s 10ms/step - loss: 43.7127 - acc: 0.0430 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 6/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 43.7302 - acc: 0.0426 - val_loss: 1.9157 - val_acc: 0.0952\n",
      "Epoch 7/10\n",
      "2700/2700 [==============================] - 27s 10ms/step - loss: 43.5245 - acc: 0.0456 - val_loss: 1.9119 - val_acc: 0.0952\n",
      "Epoch 8/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 35.5489 - acc: 0.0344 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Epoch 9/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 34.6164 - acc: 0.0304 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Epoch 10/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 34.5263 - acc: 0.0315 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Train on 2700 samples, validate on 2700 samples\n",
      "Epoch 1/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 9.5553 - acc: 0.0748 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Epoch 2/10\n",
      "2700/2700 [==============================] - 26s 9ms/step - loss: 9.5648 - acc: 0.0737 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Epoch 3/10\n",
      "2700/2700 [==============================] - 26s 9ms/step - loss: 9.5748 - acc: 0.0756 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Epoch 4/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 9.5550 - acc: 0.0748 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Epoch 5/10\n",
      "2700/2700 [==============================] - 26s 9ms/step - loss: 9.5744 - acc: 0.0741 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Epoch 6/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 9.5269 - acc: 0.0752 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Epoch 7/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 9.5631 - acc: 0.0741 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Epoch 8/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 9.5555 - acc: 0.0752 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Epoch 9/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 9.5698 - acc: 0.0737 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Epoch 10/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 9.5459 - acc: 0.0737 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Train on 2700 samples, validate on 2700 samples\n",
      "Epoch 1/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 24.7867 - acc: 0.0504 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Epoch 2/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 24.8561 - acc: 0.0478 - val_loss: 5.7469 - val_acc: 0.0348\n",
      "Epoch 3/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 24.5832 - acc: 0.0507 - val_loss: 1.6885 - val_acc: 0.1359\n",
      "Epoch 4/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 21.3291 - acc: 0.0504 - val_loss: 1.8796 - val_acc: 0.0952\n",
      "Epoch 5/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 19.6721 - acc: 0.0522 - val_loss: 1.6590 - val_acc: 0.0952\n",
      "Epoch 6/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 19.0132 - acc: 0.0581 - val_loss: 1.6055 - val_acc: 0.1074\n",
      "Epoch 7/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 18.7200 - acc: 0.0581 - val_loss: 1.6693 - val_acc: 0.1096\n",
      "Epoch 8/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 18.6510 - acc: 0.0578 - val_loss: 1.6544 - val_acc: 0.1096\n",
      "Epoch 9/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 18.6787 - acc: 0.0600 - val_loss: 1.6794 - val_acc: 0.1096\n",
      "Epoch 10/10\n",
      "2700/2700 [==============================] - 26s 10ms/step - loss: 18.6862 - acc: 0.0570 - val_loss: 1.6486 - val_acc: 0.1048\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "#训练模型\n",
    "from keras import optimizers\n",
    "import h5py\n",
    "\n",
    "img_path = params.data_dir\n",
    "batch_size = None\n",
    "input_shape=(params.FLAGS.img_w, params.FLAGS.img_h, params.FLAGS.img_c)\n",
    "model = nvidia_model(input_shape=input_shape)\n",
    "sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "               optimizer=adam,\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "with open(utils.join_dir(img_path, 'epoch{:0>2}.pkl'.format('09')),'rb') as fs:\n",
    "        x_valid,y_valid = pickle.load(fs)\n",
    "        \n",
    "for epochid in ['01','02','03','04','05','06','07','08']:\n",
    "    file_name = utils.join_dir(img_path, 'epoch{:0>2}.pkl'.format(epochid))\n",
    "    with open(file_name,'rb') as fs:\n",
    "        x_train,y_train = pickle.load(fs)\n",
    "        if(len(x_train) == 0):\n",
    "            break\n",
    "        model.fit(x_train, y_train,epochs=10,batch_size=128,validation_data=(x_valid,y_valid),shuffle=True)\n",
    "    \n",
    "utils.save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists, do you want to reuse? (y/n): y\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character '\\xe3' in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6c75b2902e01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\machinelearning\\deep_tesla\\utils.py\u001b[0m in \u001b[0;36mget_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0min_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m                 \u001b[0mjson_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[0mweights_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mmodel_from_json\u001b[1;34m(json_string, custom_objects)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \"\"\"\n\u001b[0;32m    345\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32mC:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 return cls.from_config(config['config'],\n\u001b[0;32m    139\u001b[0m                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[1;32m--> 140\u001b[1;33m                                                            list(custom_objects.items())))\n\u001b[0m\u001b[0;32m    141\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m   2488\u001b[0m         \u001b[1;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2489\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layers'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2490\u001b[1;33m             \u001b[0mprocess_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2491\u001b[0m         \u001b[1;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2492\u001b[0m         \u001b[1;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[1;34m(layer_data)\u001b[0m\n\u001b[0;32m   2474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2475\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[1;32m-> 2476\u001b[1;33m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[0;32m   2477\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32mC:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 return cls.from_config(config['config'],\n\u001b[0;32m    139\u001b[0m                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[1;32m--> 140\u001b[1;33m                                                            list(custom_objects.items())))\n\u001b[0m\u001b[0;32m    141\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mfunction_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'lambda'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Unsafe deserialization from bytecode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             \u001b[0mfunction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'function'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown function type:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mfunc_load\u001b[1;34m(code, defaults, closure, globs)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mclosure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_value_to_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[0mraw_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'base64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m     \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarshal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mglobs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character '\\xe3' in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "#测试模型\n",
    "import params,utils\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "img_path = params.data_dir\n",
    "input_shape=(params.FLAGS.img_w, params.FLAGS.img_h, params.FLAGS.img_c)\n",
    "#x_test,y_test,_,_ = VidDataGenerator(img_path,'10',batch_size=None,input_shape=input_shape).next_batch()\n",
    "with open(utils.join_dir(img_path, 'epoch{:0>2}.pkl'.format('10')),'rb') as fs:\n",
    "        x_test,y_test = pickle.load(fs)\n",
    " \n",
    "  \n",
    "\n",
    "model = utils.get_model()\n",
    "y_predict = model.predict(x_test)\n",
    "print(y_predict,y_test)\n",
    "score = r2_score(y_test,y_predict)\n",
    "print('Score:',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "rows = pd.read_csv('C:/machinelearning/0130/deep.csv')\n",
    "y1=list(rows.Loss.values)\n",
    "x1=range(0,len(y1)) \n",
    "y2=list(rows.ValLoss.values)\n",
    "x2=range(0,len(y2)) \n",
    "plt.plot(x1,y1,label='Loss') \n",
    "plt.plot(x2,y2,label='Val Loss') \n",
    "plt.xlabel('epochs Number') \n",
    "plt.ylabel('Loss') \n",
    "plt.title('0130') \n",
    "plt.legend() \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nvidia_model 视频01 epochs10  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
